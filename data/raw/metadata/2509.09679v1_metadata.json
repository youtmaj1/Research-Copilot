{
  "id": "2509.09679v1",
  "title": "ButterflyQuant: Ultra-low-bit LLM Quantization through Learnable   Orthogonal Butterfly Transforms",
  "authors": [
    "Bingxin Xu",
    "Zhen Dong",
    "Oussama Elachqar",
    "Yuzhang Shang"
  ],
  "abstract": "Large language models require massive memory footprints, severely limiting deployment on consumer hardware. Quantization reduces memory through lower numerical precision, but extreme 2-bit quantization suffers from catastrophic performance loss due to outliers in activations. Rotation-based methods such as QuIP and QuaRot apply orthogonal transforms to eliminate outliers before quantization, using computational invariance: $\\mathbf{y} = \\mathbf{Wx} = (\\mathbf{WQ}^T)(\\mathbf{Qx})$ for orthogonal $\\mathbf{Q}$. However, these methods use fixed transforms--Hadamard matrices achieving optimal worst-case coherence $\\mu = 1/\\sqrt{n}$--that cannot adapt to specific weight distributions. We identify that different transformer layers exhibit distinct outlier patterns, motivating layer-adaptive rotations rather than one-size-fits-all approaches. We propose ButterflyQuant, which replaces Hadamard rotations with learnable butterfly transforms parameterized by continuous Givens rotation angles. Unlike Hadamard's discrete $\\{+1, -1\\}$ entries that are non-differentiable and prohibit gradient-based learning, butterfly transforms' continuous parameterization enables smooth optimization while guaranteeing orthogonality by construction. This orthogonal constraint ensures theoretical guarantees in outlier suppression while achieving $O(n \\log n)$ computational complexity with only $\\frac{n \\log n}{2}$ learnable parameters. We further introduce a uniformity regularization on post-transformation activations to promote smoother distributions amenable to quantization. Learning requires only 128 calibration samples and converges in minutes on a single GPU--a negligible one-time cost. On LLaMA-2-7B with 2-bit quantization, ButterflyQuant achieves 15.4 perplexity versus 22.1 for QuaRot.",
  "doi": null,
  "published_date": "2025-09-11T17:59:51+00:00",
  "updated_date": "2025-09-11T17:59:51+00:00",
  "categories": [
    "cs.LG",
    "cs.AI",
    "cs.CL"
  ],
  "source": "arxiv",
  "pdf_url": "http://arxiv.org/pdf/2509.09679v1",
  "arxiv_url": "http://arxiv.org/abs/2509.09679v1",
  "hash": "bafc6e956b7e37b1a5cf99d64037f262"
}